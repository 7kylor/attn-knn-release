{
  "project": "Attn-KNN",
  "version": "1.0",
  "date": "2025-12",
  "status": "CONCLUDED",
  "conclusion": "Attention provides no meaningful benefit over distance-weighted kNN. TTA improves calibration. Project concluded.",
  "best_results": {
    "experiment": 7,
    "method": "Attn-KNN + TTA",
    "metrics": {
      "accuracy": 0.9099,
      "ece": 0.0267,
      "nll": 0.513
    }
  },
  "final_results_experiment_7": {
    "uniform_knn": {"accuracy": 0.9153, "ece": 0.0796, "nll": 1.225},
    "distance_knn": {"accuracy": 0.9152, "ece": 0.0783, "nll": 1.225},
    "attn_knn": {"accuracy": 0.9155, "ece": 0.0811, "nll": 1.236},
    "attn_knn_tta": {"accuracy": 0.9099, "ece": 0.0267, "nll": 0.513},
    "cnn_baseline": {"accuracy": 0.9651, "ece": 0.0253, "nll": 0.184}
  },
  "noise_robustness_30pct": {
    "uniform_knn_drop": "0.04%",
    "attn_knn_drop": "1.05%",
    "conclusion": "Attention is LESS robust to noise"
  },
  "experiments": {
    "experiment_1-3": {
      "name": "Initial Prototype",
      "config": {
        "backbone": "ResNet18",
        "embed_dim": 128,
        "num_heads": 1,
        "epochs": 30
      },
      "results": {
        "uniform_knn": {
          "accuracy": 0.8861,
          "ece": 0.0165
        },
        "distance_knn": {
          "accuracy": 0.8862,
          "ece": 0.0347
        },
        "attn_knn": {
          "accuracy": 0.8854,
          "ece": 0.0329
        }
      }
    },
    "experiment_4": {
      "name": "Architecture Improvements",
      "config": {
        "backbone": "ResNet50",
        "embed_dim": 256,
        "num_heads": 4,
        "epochs": 50,
        "contrastive_weight": 0.5,
        "hard_negative_weight": 2.0
      },
      "results": {
        "uniform_knn": {
          "accuracy": 0.8969,
          "ece": 0.0243
        },
        "distance_knn": {
          "accuracy": 0.8970,
          "ece": 0.0566
        },
        "attn_knn": {
          "accuracy": 0.8970,
          "ece": 0.0578
        },
        "cnn_baseline": {
          "accuracy": 0.9494,
          "ece": 0.0718
        }
      }
    },
    "experiment_5": {
      "name": "Enhanced Training (BEST)",
      "config": {
        "backbone": "ResNet18",
        "embed_dim": 256,
        "num_heads": 4,
        "epochs": 20,
        "batch_size": 512,
        "k_train": 16,
        "mixup_alpha": 0.4,
        "tta_augments": 5
      },
      "results": {
        "uniform_knn": {
          "accuracy": 0.8682,
          "f1": 0.8678,
          "ece": 0.1297,
          "nll": 2.053
        },
        "distance_knn": {
          "accuracy": 0.8685,
          "f1": 0.8681,
          "ece": 0.1099,
          "nll": 2.054
        },
        "attn_knn": {
          "accuracy": 0.8685,
          "f1": 0.8681,
          "ece": 0.1300,
          "nll": 2.054
        },
        "attn_knn_tta": {
          "accuracy": 0.8720,
          "f1": 0.8727,
          "ece": 0.0283,
          "nll": 0.967
        },
        "attn_knn_ensemble": {
          "accuracy": 0.8683,
          "f1": 0.8679,
          "ece": 0.1299,
          "nll": 2.047
        },
        "cnn_baseline": {
          "accuracy": 0.9512,
          "f1": 0.9511,
          "ece": 0.0685,
          "nll": 0.242
        }
      }
    },
    "experiment_6": {
      "name": "Final Validation",
      "config": {
        "same_as": "experiment_5"
      },
      "results": {
        "uniform_knn": {
          "accuracy": 0.8557,
          "f1": 0.8553,
          "ece": 0.1336,
          "nll": 2.028
        },
        "distance_knn": {
          "accuracy": 0.8555,
          "f1": 0.8551,
          "ece": 0.1169,
          "nll": 2.028
        },
        "attn_knn": {
          "accuracy": 0.8548,
          "f1": 0.8544,
          "ece": 0.1359,
          "nll": 2.058
        },
        "attn_knn_tta": {
          "accuracy": 0.8554,
          "f1": 0.8550,
          "ece": 0.0379,
          "nll": 0.931
        },
        "cnn_baseline": {
          "accuracy": 0.9497,
          "f1": 0.9496,
          "ece": 0.0726,
          "nll": 0.247
        }
      }
    }
  },
  "k_sweep": {
    "experiment": 5,
    "k_values": [
      1,
      3,
      5,
      10,
      20,
      50
    ],
    "results": {
      "k_1": {
        "uniform": 0.8682,
        "distance": 0.8682,
        "attention": 0.8682
      },
      "k_3": {
        "uniform": 0.8684,
        "distance": 0.8684,
        "attention": 0.8685
      },
      "k_5": {
        "uniform": 0.8683,
        "distance": 0.8683,
        "attention": 0.8686
      },
      "k_10": {
        "uniform": 0.8682,
        "distance": 0.8684,
        "attention": 0.8682
      },
      "k_20": {
        "uniform": 0.8683,
        "distance": 0.8683,
        "attention": 0.8685
      },
      "k_50": {
        "uniform": 0.8680,
        "distance": 0.8680,
        "attention": 0.8684
      }
    }
  },
  "noise_robustness": {
    "experiment": 5,
    "noise_rates": [
      0.0,
      0.1,
      0.2,
      0.3
    ],
    "results": {
      "noise_0.0": {
        "uniform": 0.8682,
        "distance": 0.8685,
        "attention": 0.8685
      },
      "noise_0.1": {
        "uniform": 0.8684,
        "distance": 0.8685,
        "attention": 0.8682
      },
      "noise_0.2": {
        "uniform": 0.8683,
        "distance": 0.8685,
        "attention": 0.8685
      },
      "noise_0.3": {
        "uniform": 0.8682,
        "distance": 0.8685,
        "attention": 0.8680
      }
    }
  },
  "key_findings": [
    "TTA provides 67% ECE reduction (0.0811 -> 0.0267)",
    "Attention provides only +0.02% accuracy improvement (noise level)",
    "Attention is LESS robust to label noise than uniform kNN (1.05% vs 0.04% drop)",
    "5% accuracy gap to CNN upper bound remains",
    "k parameter has minimal impact on accuracy"
  ],
  "claim_validation": {
    "calibration_improvement": "PARTIAL - Only with TTA, not attention",
    "accuracy_improvement": "INVALIDATED - +0.02% is within noise",
    "robustness": "INVALIDATED - Attention worse under noise",
    "compute_overhead": "VALIDATED - <1ms"
  },
  "recommendations": [
    "Use distance-weighted kNN with TTA for production",
    "Attention adds complexity without benefit",
    "Focus on embedding quality rather than attention mechanism",
    "No further investigation of attention-weighted kNN needed"
  ],
  "project_status": "CONCLUDED - December 2025"
}